---
title: "Assignment 2. Non-linear dimensionality reduction."
author: "Ã€lex Martorell, Enric Reverter, Pim Schoolkate"
date: "3/10/2022"
output: 
  html_document:
    toc: true
    theme: united
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## PART A. Principal Curves

### 1. Choosing the smoothing parameter in Principal Curves (Hastie and Stuetzle 1989)

The data used for the first part is the one generated by the following:
```{r}
t <- seq(-1.5*pi,1.5*pi,l=100)
R<- 1
n<-75
sd.eps <- .15

set.seed(1)
y <- R*sign(t) - R*sign(t)*cos(t/R)
x <- -R*sin(t/R)
z <- (y/(2*R))^2
rt <- sort(runif(n)*3*pi - 1.5*pi)
eps <- rnorm(n)*sd.eps
ry <- R*sign(rt) - (R+eps)*sign(rt)*cos(rt/R)
rx <- -(R+eps)*sin(rt/R)
rz <- (ry/(2*R))^2 + runif(n,min=-2*sd.eps,max=2*sd.eps)
XYZ <- cbind(rx,ry,rz)


require(plot3D)
lines3D(x,y,z,colvar = NULL, 
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=4,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```

#### Question a)

Leave-one-out cross-validation is applied with the following code, where each point is projected into the curve fitted by the other points and its distance measured as the cross-validated error.
```{r}
library(princurve)

df_loocv = function(data, df_vals) {
  cv_dist = setNames(data.frame(matrix(ncol = 2, nrow = 0)), c("df", "dist"))
  for (df in df_vals) {
    abs_dist = 0
    
    for (i in 1:nrow(data)) {
      fit = principal_curve(data[-i,], df=df)
      dist = project_to_curve(matrix(data[i,], ncol=ncol(data)), fit$s[fit$ord,])$dist
      abs_dist = abs_dist + dist
    }
    cv_dist = rbind(cv_dist, data.frame("df"=df, "dist"=abs_dist/nrow(data)))
  }
  return(cv_dist)
}

df.seq = df_loocv(XYZ, seq(2, 8, by=1))
df.seq
```

LOOCV returns *df = 6* as the best option, with an average predicted mean squared error of *0.1077976*.

#### Question b)

The graphical representation of the curve for *df = 6* seems to do a good job, as it captures well the more clustered points and the curve does not deviate from them.
```{r}
fit <- principal_curve(XYZ, df= 6)

rx1 <- fit$s[, "rx"]
ry1 <- fit$s[, "ry"]
rz1 <- fit$s[, "rz"]

lines3D(rx1,ry1,rz1,colvar = NULL, 
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=4,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```

#### Question c)

The **df** parameter is the number of degrees of freedom of the curve fitting. This does not really have an immediate interpretation (like the degrees of freedom in a linear system of equations), but it is easy that the more degrees of freedom we consider, the curve becomes more a segment to segment connection of the different points.

```{r}
d.50 = df_loocv(XYZ, 50)
d.50
```

The average distance between the points using LOOCV and projected curve is approximately *0.03939495*, significantly lower than a curve fitting with *df = 6*. However, if we plot the fitted curve with the original points, overfitting can be clearly observed.

```{r}
fit <- principal_curve(XYZ, df= 50)

rx1 <- fit$s[, "rx"]
ry1 <- fit$s[, "ry"]
rz1 <- fit$s[, "rz"]


lines3D(rx1,ry1,rz1,colvar = NULL, 
         phi = 20, theta = 60, r =sqrt(3), d =3, scale=FALSE,
         col=2,lwd=4,as=1,
         xlim=range(rx),ylim=range(ry),zlim=range(rz))
points3D(rx,ry,rz,col=4,pch=19,cex=.6,add=TRUE)
```

This is because the model does not penalize complexity, so if we increase the degrees of freedom in the fitting, the distance will decrease. This comes at the cost of overfitting. If we added a penalization term to the loss function (usually stated $\lambda$) overfitting could be detected.

## PART B. Local MDS, ISOMAP and t-SNE

### 2. Local MDS for ZERO digits

Reading the train ZIP set and select the zero rows.
```{r}
zip.train <- read.table("zip.train.txt")
dim(zip.train)
zip.train.0 <- zip.train[zip.train$V1 == 0, ]
```

```{r, echo=FALSE}
plot.zip <- function(x,use.first=FALSE,...){
  x<-as.numeric(x)
  if (use.first){
    x.mat <- matrix(x,16,16)
  }else{
    x.mat <- matrix(x[-1],16,16)
  }
  image(1:16,1:16,x.mat[,16:1],
        col=gray(seq(1,0,l=12)),...)
  invisible(
    if (!use.first){
      title(x[1])
    }else{
    }
  )  
  #col=gray(seq(1,0,l=2)))
}
```

#### Question a)

```{r}
library(stops)

# Remove the row "names"
zip.train.0 <- zip.train.0[, -1]

# Computes the euclidean distance between points
zip.train.0.dist <- dist(zip.train.0)

# Check dimension of distance matrix
dim(zip.train.0)
n <- dim(zip.train.0)[1]

k <- 5
tau <- .05
q<-2 # 2-dim config

conf0 <- cmdscale(zip.train.0.dist, k=q)$points

# takes a long time to run
lmds.S.res <- lmds(as.matrix(zip.train.0.dist), init=conf0, ndim=q, k=k, tau=tau, itmax = 1000)

conf.lmds.S.res <- lmds.S.res$conf

names(lmds.S.res)

par(mfrow=c(1,1))
plot(conf.lmds.S.res,as=1, main=paste0("Local MDS, k=",k,", tau=",tau))
# text(conf.lmds.S.res[,1],conf.lmds.S.res[,2],1:n,pos=4)
```

#### Question b)

To cover the whole variability of the plot using 9 points, consider a 3x3 grid where the points are computed using three statistical parameters: The minimum, the maximum and the median. Since these abstract points are not likely to belong to the 2D-configuration, we find the closest point with the Euclidean metric.

```{r}
find.9.points <- function(points) {
  p1 <- c(min(points[,1]), min(points[,2]))
  p2 <- c(min(points[,1]), median(points[,2]))
  p3 <- c(min(points[,1]), max(points[,2]))
  p4 <- c(median(points[,1]), min(points[,2]))
  p5 <- c(median(points[,1]), median(points[,2]))
  p6 <- c(median(points[,1]), max(points[,2]))
  p7 <- c(max(points[,1]), min(points[,2]))
  p8 <- c(max(points[,1]), median(points[,2]))
  p9 <- c(max(points[,1]), max(points[,2]))
return(c(p1,p2,p3,p4,p5,p6,p7,p8,p9))
}
```

```{r}
plot.9.points <- function(points, main=NULL) {
  pts <- find.9.points(points)
  print(pts)
  par(mfrow=c(3,3))
  
  for (i in 1:9 ){
    min = 1000
  
    for (j in 1:nrow(points)){
      dist <- sqrt((points[j,1]-pts[2*i-1])^2 + (points[j,2]-pts[2*i])^2 )
      if(dist < min){
        min = dist
        idx <- j
        }  
    }
    print(idx)
    plot.zip(zip.train.0[idx, ], use.first = TRUE, main=paste(as.character(i), ":", as.character(idx)))
  
  }
  par(mfrow=c(1,1))
  mtext(main, side = 3, line = -1, outer = TRUE)
}
```

```{r}
plot.9.points(conf.lmds.S.res)
```

Notice the differences in the zeroes plotted. Dimension 1 seems to portray the overall area of the circle and dimension 2 the width of tracing of the drawing. (For higher values in the second dimension, the trace is wider).

#### Question c)

The objective of this question is to tune parameters $\tau$ and $K$. Recall that $\tau$ is a parameter used in the weighting of larger distances between points (less important in local MDS). $K$ is the size of the neighborhood to consider for each point in the Data set. 
Chen and Buja (2009) propose to compare the neighborhood points of an observation in the high-dimensional and in the low one. As explained in the Theory slides, the quantity $N_{K'}$ (later normalized as $M_{K'}$ so it takes values between 0 and 1) is a measure of the number of $K'$-nearest neighbors that are both in the $K'$-nearest neighborhood sets defined in the distance $D$ matrix (size $n \times n$) and in the the euclidean configuration $X$ (size 
$n \times q$)
$$ N_{K'}(i) = |N^D_{K'}(i) \cap N^X_{K'}(i)|$$ 
This whole process of overlapping in the two spaces is referred to as **Local Continuity Criteria**.

The LCMC function below computes this aforementioned $N_{K'}$ ($M_{K'}$ in particular)
```{r}
LCMC <- function(D1,D2,Kp){
  D1 <- as.matrix(D1)
  D2 <- as.matrix(D2)
  n <- dim(D1)[1]
  N.Kp.i <- numeric(n)
  for (i in 1:n){
    N1.i <- sort.int(D1[i,],index.return = TRUE)$ix[1:Kp]
    N2.i <- sort.int(D2[i,],index.return = TRUE)$ix[1:Kp]
    N.Kp.i[i] <- length(intersect(N1.i, N2.i))
  }
  N.Kp<-mean(N.Kp.i)
  M.Kp.adj <- N.Kp/Kp - Kp/(n-1)
  
  return(list(N.Kp.i=N.Kp.i, M.Kp.adj=M.Kp.adj))
}
```

The goal is to find $\tau$ and $K$ that maximize the Local Continuity criteria. Following one of the strategies used by the authors, different values of the parameters are checked for different values of the free parameter $Kp$.
```{r}
D1 <- zip.train.0.dist
q <- 2

conf0 <- cmdscale(D1,k=q)$points

K <- c(5,10,50)
tau <- c(.1,.5,1)

LCMC_Ks = function(D1, Kps, K, tau, itmax) {

  LC <- matrix(0,nrow=length(K),ncol=length(tau))
  lmds.k.tau <- array(vector("list",1),dim=dim(LC))
  kps.results = setNames(data.frame(matrix(ncol=4, nrow = 0)), c("Kp", "K", "tau", "M.Kp.adj"))
  
  for (k in Kps) {
    for (i in 1:length(K)){
      for (j in 1:length(tau)){
        lmds.k.tau[[i,j]] <- lmds(as.matrix(D1), init=conf0, 
                                  ndim=q, k=K[i], tau=tau[j], itmax=itmax)$conf
        D2.k.tau <- dist(lmds.k.tau[[i,j]])
        LC[i,j] <- LCMC(D1,D2.k.tau,k)$M.Kp.adj
        print(c(i,j,LC[i,j]))
        
        kps.results = rbind(kps.results, data.frame("Kp"=k, "K"=K[i], "tau"=tau[j], "M.Kp.adj"=LC[i,j]))
      }
    }
  }
  
  ij.max <- arrayInd(which.max(LC),.dim=dim(LC))
  k.max <- K[ij.max[1]] 
  tau.max <- tau[ij.max[2]] 
  lmds.max <- lmds.k.tau[[ij.max[1],ij.max[2]]]
  
  # print(paste0("k.max=",k.max,"; tau.max=",tau.max))
  return(list("ij.max"=ij.max, "k.max"=k.max, "tau.max"=tau.max, "lmds.max"=lmds.max, "res.data"=kps.results))
}
```

Specifically, $Kp$ ranging from 5 to 50 by intervals of 5 is used.
```{r, eval = FALSE}
lcmc.k.res = LCMC_Ks(D1, seq(5,50,by=5), K, tau, itmax=100)
```

```{r}
library(ggplot2)
theme_set(theme_bw())
load("lmds_lcmc_res.RData")

ggplot(data=lcmc.k.res$res.data, aes(x=Kp, y=M.Kp.adj)) + geom_line(aes(colour=as.factor(K)))
```

Hence, it can be observed the best parameters for LMDS are consistently $K=5$ and $\tau=1$. Also, it can be noticed how varying the value of $Kp$ does not really effect the obtained results. Henceforth, for the next algorithms, $Kp$ is fixed to 10 as in the laboratory sessions.

Finally, we try to describe graphically the low dimensional configuration with the optimal parameters.
```{r}
lmds.k.max = lmds(as.matrix(D1), init=conf0, ndim=q, k=5, tau=1, itmax=1000)$conf
```

```{r}
plot(lmds.k.max, main=paste0("Local MDS, k=",5,", tau=",1))
```

Note that in comparison with other examples, it is not possible to plot the reduced dimensions against the 256 found in the original data.

### 3. ISOMAP for ZERO digits

The same procedure is applied for ISOMAP.

#### Question a)

```{r}
library(vegan)
ismp <- isomap(zip.train.0.dist, k=5, ndim=2)

par(mfrow=c(1,1))
aux.plot <- plot(ismp,n.col=3,main="Output of ISOMAP Algorithm")
points(aux.plot,"sites",pch=19,cex=.6)
```

#### Question b)

```{r}
plot.9.points(ismp$points)
```

Notice the differences in the zeroes plotted. For low values of the second dimension, the zeroes seem to not be written in a proper circle shape.
Also, as the first dimension increases, the zeroes seem to gain area in its interior. 
So, as the second dimension grows zeroes look more like circles, while as the first dimensions increases, they grow in size.

#### Question c)

LCMC can also be applied to tune the parameter $k$ of ISOMAP. This time, however, only one $Kp$ has been considered, with its value equal to 10.
```{r}
q <- 2
K <- c(5,10,50)

LC <- numeric(length(K))
ISOMAP.k <- vector("list",length(K))
  
for (i in 1:length(K)){
  ISOMAP.k[[i]] <- isomap(D1, ndim=q, 
                              k= K[i])
  D2.k <- dist(ISOMAP.k[[i]]$points[,1:q])
  LC[i] <- LCMC(D1,D2.k,10)$M.Kp.adj
  print(c(i,LC[i]))
}
  
i.max <- which.max(LC)
k.max <- K[i.max]
ISOMAP.max <- ISOMAP.k[[i.max]]
```

```{r}
plot(K, LC, type="b", main=paste0("k.max=",round(k.max,4)))
abline(v=k.max,col=2)
```

It can be observed how the best value for $k$ seems to be 10:
```{r}
ismp <- isomap(zip.train.0.dist, k=10, ndim=2)

par(mfrow=c(1,1))
plot(ismp,n.col=3,main="Output of ISOMAP Algorithm")
```

### 4. t-SNE for ZERO digits

Again, the same procedure is applied for t-SNE.

#### Question a)

```{r}
library(Rtsne)

tsne_out <- Rtsne(zip.train.0,pca=FALSE,perplexity=40,theta=0.0, dims=2) # Run TSNE

plot(tsne_out$Y, asp=1)
```

#### Question b)

```{r}
plot.9.points(tsne_out$Y)
```

It seems like as the second dimension increases, the size of the zeroes does as well. Then, a priori, it seems that as the first dimension increases, the zeroes are slightly thicker, but not much seems to be captured by such dimension.

#### Question c)

LCMC can also be applied to tune the **perplexity** of t-SNE, Again, only one $Kp$ has been considered, with its value equal to 10.
```{r}
set.seed(4444)

q <- 2
Kp <- 10

perplexity <- c(10, 20, 40) #seq(9,21,by=3)

LC <- numeric(length(perplexity))
Rtsne.k <- vector("list",length(perplexity))

for (i in 1:length(perplexity)){
    Rtsne.k[[i]] <- Rtsne(D1, perplexity=perplexity[i], dims=q,
                          theta=0, pca=FALSE, max_iter = 1000)
    D2.k <- dist(Rtsne.k[[i]]$Y)
    LC[i] <- LCMC(D1,D2.k,Kp)$M.Kp.adj
    print(paste0("perplexity:", perplexity[i], "LC:", LC[i]))
}

i.max <- which.max(LC)
perplexity.max <- perplexity[i.max[1]] 
Rtsne.max <- Rtsne.k[[i.max]]
```

```{r}
plot(perplexity,LC, main=paste0("perplexity.max=",perplexity.max))
lines(perplexity,LC, main=paste0("perplexity.max=",perplexity.max))
abline(v=perplexity[i.max],col=2)
```

It can be observed how according to the LCMC a perplexity equal to 20 seems to yield the best results.

Then, for such a perplexity, the dimensions to which data is reduced look as follows:
```{r}
tsne_out <- Rtsne(zip.train.0, pca=FALSE, perplexity=20, theta=0.0, dims=2)

plot(tsne_out$Y, asp=1)
```

### 5. Compare Local MDS, ISOMAP and t-SNE for ZERO digits

Regarding LMDS, it can be observed how the variability is much more sparse in dimension 1 (D1) than in dimension 2 (D2), while in ISOMAP and t-SNE both dimensions have similar sparsity when keeping the aspect ratio equal to 1. Also, points seem to be equally closer in LMDS, as some points in ISOMAP fall far from the cloud. For t-SNE, points are clustered in smaller groups.
```{r}
library(patchwork)
theme_set(theme_bw())

lmds.points = as.data.frame(lmds.k.max)
p1 = ggplot(data=lmds.points, aes(x=D1, y=D2)) + geom_point(size=0.5) +
  labs(title="LMDS", x ="Dimension 1", y = "Dimension 2")
p1 + coord_equal()

isomap.points = as.data.frame(ISOMAP.max$points)
p2 = ggplot(data=isomap.points, aes(x=Dim1, y=Dim2)) + geom_point(size=0.5) + 
  labs(title="ISOMAP", x ="Dimension 1", y = "Dimension 2")
p2 + coord_fixed()

tsne.points = as.data.frame(tsne_out$Y)
p3 = ggplot(data=tsne.points, aes(x=V1, y=V2)) + geom_point(size=0.5) + 
  labs(title="t-SNE", x ="Dimension 1", y = "Dimension 2")
p3 + coord_fixed()
```

It can be noted how all the clouds resemble a similar shape, though. For instance, if we do not keep the aspect ratio of 1 and we invert some of the axis the following is observed:
```{r}
p1 / (p2 + scale_y_continuous(trans = "reverse")) / (p3 + coord_flip())
```

So, using LMDS as the reference, ISOMAP resembles it if its D2 is inverted, and t-SNE also resembles it when flipping its D1 and D2.

Looking at the bivariate plot between the different lower space dimensions we obtain the following:
```{r}
final.dims = cbind(lmds.points, isomap.points, tsne.points)
colnames(final.dims) = c("lmds.d1", "lmds.d2", "isomap.d1", "isomap.d2", "tsne.d1", "tsne.d2")

pairs(final.dims)
```

That is, the seemingly observed correlations in the previous plots are confirmed. For instance, lmds.d1 appears to be very similar to isomap.d1 and tsne.d2, while lmds.d2 appears to be capture the same variance as isomap.d2 and tsne.d1.

Now, looking again at the points that capture most of the variability for such clouds:
```{r}
plot.9.points(lmds.k.max, "LMDS")
plot.9.points(ISOMAP.max$points, "ISOMAP")
plot.9.points(tsne_out$Y, "t-SNE")
```

Again, using LMDS as the reference, it shows how zeroes grow in size along D1 while they lose the circular shape as D2 increases. For ISOMAP, it holds true that they seem to grow in size along D1, but since D2 seems to be captured the other way around as in LMDS, the circular shape seems to be gained as D2 decreases. For t-SNE, size of zeroes grows along D2 while not much can be said about D1 besides thickness seems to increase.

So, the different dimensional reduction techniques capture similar variances, but they are represented differently across their dimensions. Remember that Local-MDS functions with the euclidean distance between points while ISOMAP calculates geodesics between neighborhoods of a point. However, t-SNE does not seem to capture that well differences between zeroes.

However, looking at the results from the LCMC for *kp=10* and the best parameters for each method:
```{r}
kp=10

lmds.M.Kp.adj = LCMC(D1,dist(lmds.points),kp)$M.Kp.adj
isomap.M.Kp.adj = LCMC(D1,dist(isomap.points),kp)$M.Kp.adj
tsne.M.Kp.adj = LCMC(D1,dist(tsne.points),kp)$M.Kp.adj

paste("LMDS:", lmds.M.Kp.adj)
paste("ISOMAP:", isomap.M.Kp.adj)
paste("TSNE:", tsne.M.Kp.adj)
```

t-SNE produces the 2-dimensional configuration with the highest value of the local continuity meta criteria, probably due to the fact that points are more clustered in its results.
